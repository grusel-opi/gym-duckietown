
\chapter{Vorgehensweise}

\section{Netzarchitektur}

Als Basismodell dient uns das von Nvidia entworfene CNN zur Schätzung eines Steuerbefehls für ein Fahrzeug \cite{nvidia}. Dieses Netz nutzt fünf Faltungsschichten zur Extraktion von Merkmalen und drei Dense-Schichten als Regressor.

Unsere Variante dieses Modells nutzt statt der ReLu die ELU als Aktivierungsfunktion. Diese soll den Lernprozess zusätzlich beschleunigen und die Fähigkeit des Netzes zu Generalisieren verbessern \cite{elu}. Zusätzlich haben wir eine L2 Regularisierung für die Gewichte aller Schichten und Dropout jeweils nach den letzten beiden Faltungsschichten hinzugefügt.

In der Vorverarbeitung der Daten wird die Größe der Bilder von $(640, 480, 3)$ (im Format ($H$, $W$, $C$)) auf $(120, 160, 3)$ reduziert und anschließend das obere Drittel des Bildes abgeschnitten, da dies meißt nur Horizont und sonstige Objekte beinhaltet. Die endgültige Größe des Inputs ist dann $(80, 160, 3)$.

Eine Darstellung unseres Netzes mitsamt Vorverarbeitung der Daten ist in \ref{nvidia-model} zu sehen.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\textwidth]{kapitel4/images/drawio.png}
	\caption{Unsere Variante des Nvidia Modells}
	\label{nvidia-model}
	\vspace{0.2cm}
\end{figure}


\section{Datengewinnung}

Da wir uns in einer simulierten Umgebung befinden, gestaltet sich die Gewinnung von Daten mit entsprechenden Labeln als unproblematisch.

\subsection{Label}

Wir verfolgen zwei Ansätze in welchen wir das Netz auf unterschiedliche Ausgaben trainieren. Neben den beiden Werten, auf deren Berechnung im Folgenden eingegangen wird, nutzen wir noch einen dritten Wert als Label, die Art der Kachel auf welcher die Aufnahme entstanden ist.

Im ersten Ansatz ist die Aufgabe des Netzes die Schätzung der Pose des Agenten. Dazu wird vom Simulator eine Aufnahme erzeugt, die Pose des Agenten relativ zur Fahrbahn berechnet und als Label zur Aufnahme abgespeichert. Die Poseninformationen bestehen aus der Distanz zur rechten Fahrbahnmarkierung und der Differenz der Orientierungen von Agent und Tangente der Ideallinie. Mit einem einfachen PD-Regler kann dann ein Steuerbefehl für den Agenten berechnet werden, wobei die Geschwindigkeit fix gewählt wird und die Winkelgeschwindigkeit das Ergebnis des Reglers ist. Da der Distanzwert $d$ von uns als hundertstel Kachelgröße und die Winkeldifferenz in Grad festgelegt wurde, hat eine typische Berechnung folgende Form:\\

\begin{minipage}{\linewidth}
	\begin{lstlisting}[caption={Berechnung eines Steuerbefehls mit PD-Regler}, language=python]
	d, a = environment.cheatmodul.get_lane_pose()
	d_hat_to_center = (d - 20.5) / 100.
	a_hat_in_rad = (a * 2 * np.pi) / 360.
	steering = k_p * d_hat_to_center + k_d * a_hat_in_rad
	speed = 0.2
	observation = environment.step((speed, steering))
	data.append((observation, (d, a)))
	\end{lstlisting}
\end{minipage}

\vspace{.5cm}
Die magische Zahl von 20,5 ist einfach die Verschiebung des Wertes vom rechten Fahrbahnrand zur Mitte der Fahrspur, so dass das Ergebnis als Fehler zum Soll (also null) interpretiert werden kann. Das Teilen durch hundert ergibt dann einen Wert in Kachelgrößen, statt hundertstel Kachelgröße.
Zur Vollständigkeit ist im Code noch stark vereinfacht verdeutlicht, wie Aufnahme und zugehöriges Label zustande kommen und gespeichert werden.

Das Schätzen der Pose hat den Vorteil, dass die Informationen anderweitig verwendet werden könnten, beispielsweise zur Lokalisierung des Agenten in einem Monte-Carlo Verfahren.\\

Im zweiten Ansatz trainieren wir das Netz direkt auf entsprechende Steuerbefehle in der Form Geschwindigkeit und Winkelgeschwindigkeit $(v, \omega)$. Dazu benutzen wir ein einfaches Expertensystem, welches mehr Informationen in die Berechnung des Befehls miteinbezieht, als nur die relative Pose des Agenten zum aktuellen Zeitpunkt. Dabei wird ein in einer definierten Distanz voraus liegender Punkt auf der Ideallinie gesucht. Befindet sich dieser Punkt in einer Kurve, oder sind die Orientierungen von Agent und Tangente der Fahrspur zu verschieden, wird $v$ als die Hälfte einer festgelegten Referenz-Geschwindigkeit festgelegt. Bei Geraden ist $v$ die volle Referenz-Geschwindigkeit. Die Winkelgeschwindigkeit $\omega$ ist das Skalarprodukt aus dem Vektor, welcher von Roboter in Richtung voraus liegenden Punkt zeigt und dem Einheitsvektor, welcher vom Agenten aus nach rechts zeigt.

\begin{minipage}{\linewidth}
	\begin{lstlisting}[caption={Berechnung eines Steuerbefehls mit einfachem Expertensystem}, language=python]
	
	projected_angle, closest_point, curve_point = environment._get_projected_angle_difference(lookup_distance)
	tile = environment.get_tile(curve_point)
	if 'curve' in tile['kind'] or abs(projected_angle) < 0.92:
		v *= 0.5
	point_vec = curve_point - environment.cur_pos
	point_vec /= np.linalg.norm(point_vec)
	right_vec = np.array([math.sin(self.env.cur_angle), 0, math.cos(self.env.cur_angle)])
	omega = np.dot(right_vec, point_vec)
	
	\end{lstlisting}
\end{minipage}

\subsection{Observationen}



\section{Inferenz der Pose des Agenten}

\subsection{Datengewinnung}

Zur Erzeugung der Trainingsdaten wird in jedem \texttt{step}-Aufruf des Simulators das dazugehörige Kamerabild der DuckieBots abgegriffen, mit dem korrespondierenden Label versehen und anschließend abgespeichert. Das Label beinhaltet hierbei folgende Informationen:

\begin{enumerate}
	\item Kürzester Abstand $d$ zur rechten Fahrbahnmarkierung
	\item Winkeldifferenz $\Theta$ zwischen Orientierung des DuckieBots und der Fahrbahn
	\item Name der Kachelart, auf der sich der DuckieBot zur Zeit der Aufnahme des Kamerabildes befindet
\end{enumerate}

Die Labelinformationen können dabei direkt aus dem DuckieTown-Simulator (mit Hilfe des \glqq Cheat-Moduls\grqq) entnommen bzw. berechnet werden. Des Weiteren kann gesteuert werden, wie viele Kamerabilder aufgenommen werden sollen. \\

Im folgenden werden wir auf zwei verschiedene Ansätze eingehen, die wir zur Gewinnung der Daten verfolgt haben:

\subsubsection{Ansatz 1 - PD-Fahrt ohne Neuplatzierung des Agenten:}

Der DuckieBot wird mittels eines PD-Reglers gesteuert, so dass er die Mitte der rechten Fahrspur hält und den Streckenverlauf der Fahrbahn verfolgt. Dabei wird der DuckieBot auf einer zufälligen befahrbaren Kachel der Umgebung platziert. Die Orientierung des DuckieBots  wird hierbei ebenfalls zufällig ausgewählt.

\subsection{Ansatz 2 - PID-Fahrt mit Neuplatzierungen des Agenten:}

Bei diesem Ansatz wird der DuckieBot ebenfalls mit Hilfe eines PID-Reglers (wie in Ansatz 1 beschrieben) gesteuert. Der Unterschied hierbei ist, dass der DuckieBot nach einer gewissen Anzahl von \texttt{step}-Aufrufen des Simulators neu platziert wird. Die Platzierung des DuckieBots erfolgt hierbei ebenfalls zufällig, nach dem im Ansatz 1 beschriebenen Verfahren.

\section{Datengewinnung - Direkte Inferenz des Steuerbefehls}

Die Gewinnung der Kamerabilder des DuckieBots läuft hierbei analog, wie im Abschnitt \ref{collect-data} beschrieben ab. Die abgegriffenen Kamerabilder werden anschließend mit folgenden Informationen gelabelt:

\begin{enumerate}
	\item Geschwindigkeit $v$
	\item Winkelgeschwindigkeit $\omega$
	\item Name der Kachelart, auf der sich der DuckieBot zur Zeit der Aufnahme des Kamerabildes befindet
\end{enumerate}


Der optimale Steuerbefehl (bestehend aus $v$ und $\omega$) wird hierbei durch ein sogenanntes Expertensystem zu Verfügung gestellt. Der Name der Kachelart, auf der sich der DuckieBot zur Zeit der Aufnahme befindet, kann wieder direkt aus dem Simulator abgegriffen werden. \\

Hierbei haben wir ebenfalls zwei verschiedene Ansätze zur Gewinnung der Daten verfolgt, auf die wir im folgenden eingehen werden:

\subsection{Ansatz 1 - Expertenfahrt ohne Neuplatzierung des Agenten:}

Der DuckieBot wird mit Hilfe des vom Expertensystem vorhergesagten Steuerbefehls gesteuert. Er wird hierbei ebenfalls zu Beginn auf einer zufälligen Kachel platziert, mit einer ebenfalls zufällig ausgewählten Orientierung.

\subsection{Ansatz 2 - Expertenfahrt mit Neuplatzierung des Agenten:}

Bei diesem Ansatz wird der DuckieBot ebenfalls mit dem vom Expertensystem vorhergesagten Steuerbefehls navigiert. Jedoch wird nach einer gewissen Anzahl von \texttt{step}-Aufrufen des Simulators, eine zufällige Neuplatzierung des DuckieBots vorgenommen.


\section{Netzwerkarchitektur}

Die entworfene Netzwerkarchitektur ist sowohl für die Inferenz der Pose des Agenten als auch für die direkte Inferenz des Steuerbefehls identisch.
Für die Realisation des Netzwerkes haben wir \href{https://www.tensorflow.org/}{TensorFlow} verwendet. TensorFlow ist ein von Google entwickeltes Framework für maschinelles Lernen und Künstliche Intelligenz \cite{bigdata}. \\

Unsere Netzwerkarchitektur besteht aus 12 Schichten: einer Normalisierungsschicht, fünf Faltungsschichten, 2 Dropoutschichten, einer Flattenschicht und drei Fully-Connected-Schichten.
Dabei haben wir uns an einer bereits vorhanden Netzwerkarchitektur eines NVIDIA-Papers orientiert. Der Titel des Paper lautet: \glqq End to End Learning for Self-Driving Cars\grqq{} und kann unter \href{https://arxiv.org/pdf/1604.07316.pdf}{https://arxiv.org/pdf/1604.07316.pdf} gefunden werden.
Eine schematische Darstellung der Netzarchitektur ist in Abbildung \ref{network-architecture} dargestellt. \\

Im folgenden werden wir den Aufbau der Netzarchitektur näher erläutern:

\subsection{Normalisierungsschicht}
Die Grundidee der Normalisierungsschicht besteht darin, die Ausgabe einer Aktivierungsschicht zu normalisieren, wodurch die Konvergenz während des Trainingsprozesses verbessert wird. \cite{tensorflow}

\subsection{Faltungsschichten}
Nach der Normalisierungsschicht folgen die einzelnen Faltungsschichten. Diese sind für die Extraktion von Bildmerkmalen zuständig. \\

Die erste Faltungsschicht nutzt hierbei 24 verschiedene Filter (Kernel) der Größe 5x5 mit einer 2x2 Schrittbreite (stride) unter der Berücksichtigung der Bildränder (padding). Als Aktivierungsfunktion wird die ELu verwendet. Des Weiteren wird ein L2-Kernel-Regularizer eingesetzt, damit das Overfitting des Modells minimiert wird \cite{tensorflow2}.  \\ 
Das 5x5 große Fenster fährt dann über das Eingabebild und wendet alle 24 Filter auf den momentanen Bildausschnitt an. Jeder dieser Filter konzentriert sich dabei auf ein bestimmtes Bildmerkmal. \\

Die zweite Faltungsschicht nutzt 36 verschiedene Kernel. Die restlichen Parameter sind identisch zur ersten Faltungsschicht. \\

Die dritte Faltungsschicht nutzt 48 verschiedene Filter. Auch hier sind die restlichen Parameter identisch zur ersten Faltungsschicht. \\

Die vierte Faltungsschicht nutzt 64 verschiedene Kernel der Größe 3x3 mit einer 1x1 Schrittbreite, jedoch werden hier die Bildränder nicht berücksichtigt. Die restlichen Parameter sind wieder identisch zur ersten Faltungsschicht. \\

Die fünfte und letzte Faltungsschicht ist identisch zur vierten Faltungsschicht.

\subsection{Flattenschicht}

Nach den Faltungsschichten folgt eine sogenannte Flattenschicht, die dafür zuständig ist, die mehrdimensionale Ausgabe der Faltungsschichten  in eine eindimensionale Repräsentation zu überführen. \cite{hhu}

\subsection{Dropoutschichten}

Zwischen der vierten und der fünften Faltungsschicht befindet sich eine Dropoutschicht, sowie nach der Flattenschicht. \\ Die Dropoutschichten dienen zur Minimierung der Gefahr einer Überanpassung des Modells an die Trainingsdaten. Dabei wird beim Training des Modells eine vorher spezifizierte Anzahl von Neuronen ausgeschaltet, die dann im kommenden Berechnungsschritt nicht berücksichtigt werden. \cite{wiki2}

\subsection{Fully-Connected-Schichten}

Nach den Faltungsschichten folgen anschließend drei Fully-Connected-Schichten die uns schlussendlich die geschätzte Distanz $d$ und geschätzte Winkeldifferenz $\Theta$ liefern.
\vspace{0.6cm}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{kapitel4/images/network_architecture.png}
	\caption{Schematische Darstellung der Netzwerkarchitektur}
	\label{network-architecture}
	\vspace{0.2cm}
	\quelle\url{https://d3i71xaburhd42.cloudfront.net/0e3cc46583217ec81e87045a4f9ae3478a008227/5-Figure4-1.png}
\end{figure}

Das Netzwerk nimmt das Kamerabild des DuckieBots als Eingabe entgegen, wobei das obere drittel des Kamerabildes entfernt wurde. Dies hat den Vorteil, dass somit nur die relevanten Informationen für das Netzwerk im Kamerabild enthalten sind, wodurch sich das Training des Netzwerks verbessert. Das Netzwerk verarbeitet dann das eingehende Kamerabild und liefert anschließend die geschätzte Pose, bestehend aus dem kürzestem Abstand $d$ zur rechten Fahrbahnmarkierung sowie der Winkeldifferenz $\Theta$ zwischen Orientierung des DuckieBots und der Fahrbahn.


\section{Lernprozess}

Für den Lernprozess wird das überwachte Lernen als Lernverfahren eingesetzt. Zu Beginn des Lernprozesses werden zunächst die Hyperparameter des Modells festgelegt:
\vspace{-0.4cm}
\begin{center}
	\begin{tabular}[t]{|l|l|}
		\hline
		\textbf{Lerngeschwindigkeit} & 0.0002 \\
		\hline
		\textbf{Batch Größe} & 32 \\
		\hline
		\textbf{Optimizer} & Adam \\
		\hline
		\textbf{Anzahl Epochen} & 50 \\
		\hline
		\textbf{Verlustfunktion} & MSE \\
		\hline
	\end{tabular}
\end{center}

Anschließend wird einer der zuvor vorbereiteten Datensätze geladen, womit dann das Training des Modells erfolgt. \\

Der Hauptmechanismus des Trainings stellt eine Feedback-Schleife dar. Die grundlegende Idee dabei ist, dass wir dem Modell die aufgenommenen Kamerabilder zeigen und uns die Vorhersagen des Modells merken. Anschließend können wir die Korrektheit der erhalten Vorhersagen prüfen und den Fehler im Bezug zum tatsächlichen Wert ermitteln. Daraufhin erfolgt im Modell eine Anpassung der einzelnen Gewichte der Neuronen (durch den Backpropagation-Mechanismus), mit dem Ziel bessere Vorhersagen treffen zu können. Anschließend wird der Vorgang wiederholt. Ein einzelner Durchlauf der  Feedback-Schleife wird dabei als eine Epoche bezeichnet. Je größer die Anzahl der Epochen eines künstlichen neuronalen Netzes ist, desto länger wurde es trainiert. Zum Schluss wird das trainierte Modell abgespeichert, damit es in den Simulator integriert werden kann.

